Total combinations to train: 15

=== Training combination 1/15 ===
Steps: 15000, N_samples: 30000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 18:09:38.055065406 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:09:38.068899641 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:09:38.068927302 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 18:09:39.699065443 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:09:39.717163570 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:09:39.717214231 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 18:09:40.754487525 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:09:40.773645306 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:09:40.773671307 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 18:09:41.838140395 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:09:41.852000262 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:09:41.852025992 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 18:09:43.212082051 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:09:43.230164847 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:09:43.230209118 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~30000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~30000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 14999/15000 [04:51<00:00, 51.40it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1499/1500.0 [02:40<00:00,  9.32it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1499/1500.0 [02:38<00:00,  9.45it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7149999737739563
Final Model Recall: 0.4300000071525574
Final Model False Positives per Hour: 0.0
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)
  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Collecting tensorboard~=2.19.0 (from tensorflow)
  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)
Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)
Installing collected packages: protobuf, tensorboard
  Attempting uninstall: protobuf
    Found existing installation: protobuf 6.31.1
    Uninstalling protobuf-6.31.1:
      Successfully uninstalled protobuf-6.31.1
  Attempting uninstall: tensorboard
    Found existing installation: tensorboard 2.20.0
    Uninstalling tensorboard-2.20.0:
      Successfully uninstalled tensorboard-2.20.0
Successfully installed protobuf-5.29.5 tensorboard-2.19.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754072474.716522  107735 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754072474.723313  107735 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754072474.741737  107735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754072474.741767  107735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754072474.741773  107735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754072474.741778  107735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  127428975446352: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  127428975446160: TensorSpec(shape=(), dtype=tf.float32, name=None)
  127428975447888: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  127428975448080: TensorSpec(shape=(), dtype=tf.float32, name=None)
  127428975450768: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  127428975448656: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  127428975448272: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  127428975445008: TensorSpec(shape=(), dtype=tf.float32, name=None)
  127428975448848: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  127428975451344: TensorSpec(shape=(), dtype=tf.float32, name=None)
  127428975450384: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  127428975449040: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  127428975446544: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  127428975448464: TensorSpec(shape=(), dtype=tf.float32, name=None)
  127428975450192: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754072479.356642  107735 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754072479.356865  107735 single_machine.cc:374] Starting new session
W0000 00:00:1754072479.414256  107735 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754072479.414295  107735 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754072479.475165  107735 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754072479.475332  107735 single_machine.cc:374] Starting new session
W0000 00:00:1754072479.522134  107735 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754072479.522171  107735 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 2/15 ===
Steps: 15000, N_samples: 35000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 18:21:30.171252842 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:21:30.189878248 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:21:30.189910051 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 18:21:31.714028605 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:21:31.728438231 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:21:31.728469603 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 18:21:32.701857784 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:21:32.714664821 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:21:32.714683802 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 18:21:33.832279717 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:21:33.847055957 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:21:33.847081739 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 18:21:35.236108096 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:21:35.251338036 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:21:35.251372338 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~35000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~35000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 14999/15000 [05:06<00:00, 48.95it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1499/1500.0 [02:44<00:00,  9.09it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1499/1500.0 [02:43<00:00,  9.16it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7177500128746033
Final Model Recall: 0.43549999594688416
Final Model False Positives per Hour: 0.0
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754073186.989607  115754 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754073186.996663  115754 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754073187.017357  115754 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754073187.017389  115754 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754073187.017395  115754 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754073187.017400  115754 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  139971586574672: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  139971586574480: TensorSpec(shape=(), dtype=tf.float32, name=None)
  139971586576208: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  139971586576400: TensorSpec(shape=(), dtype=tf.float32, name=None)
  139971586579088: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  139971586576976: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  139971586576592: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  139971586573328: TensorSpec(shape=(), dtype=tf.float32, name=None)
  139971586577168: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  139971586579664: TensorSpec(shape=(), dtype=tf.float32, name=None)
  139971586578704: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  139971586577360: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  139971586574864: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  139971586576784: TensorSpec(shape=(), dtype=tf.float32, name=None)
  139971586578512: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754073191.691643  115754 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754073191.691902  115754 single_machine.cc:374] Starting new session
W0000 00:00:1754073191.746420  115754 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754073191.746466  115754 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754073191.809826  115754 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754073191.810018  115754 single_machine.cc:374] Starting new session
W0000 00:00:1754073191.857347  115754 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754073191.857392  115754 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 3/15 ===
Steps: 15000, N_samples: 40000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 18:33:22.445582881 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:33:22.459413987 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:33:22.459433668 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 18:33:23.913638653 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:33:23.928445868 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:33:23.928476189 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 18:33:25.013581675 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:33:25.026566148 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:33:25.026585559 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 18:33:26.190614916 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:33:26.204362146 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:33:26.204378947 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 18:33:27.485967895 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:33:27.500739297 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:33:27.500772199 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~40000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~40000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 14999/15000 [05:14<00:00, 47.71it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1499/1500.0 [02:47<00:00,  8.97it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1499/1500.0 [02:52<00:00,  8.69it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7365000247955322
Final Model Recall: 0.47350001335144043
Final Model False Positives per Hour: 0.0
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754073917.979029  123832 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754073917.985799  123832 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754073918.004181  123832 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754073918.004208  123832 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754073918.004213  123832 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754073918.004219  123832 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  125415062898832: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  125415062899216: TensorSpec(shape=(), dtype=tf.float32, name=None)
  125415062900752: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  125415062901328: TensorSpec(shape=(), dtype=tf.float32, name=None)
  125415062899024: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  125415062900944: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  125415062901520: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  125415062901904: TensorSpec(shape=(), dtype=tf.float32, name=None)
  125415062899408: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  125415062901712: TensorSpec(shape=(), dtype=tf.float32, name=None)
  125415062902096: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  125415062899984: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  125415062903248: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  125415062896912: TensorSpec(shape=(), dtype=tf.float32, name=None)
  125415062903056: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754073922.466905  123832 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754073922.467158  123832 single_machine.cc:374] Starting new session
W0000 00:00:1754073922.520964  123832 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754073922.521006  123832 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754073922.583430  123832 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754073922.583615  123832 single_machine.cc:374] Starting new session
W0000 00:00:1754073922.636391  123832 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754073922.636434  123832 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 4/15 ===
Steps: 20000, N_samples: 30000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 18:45:33.259695808 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:45:33.273922509 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:45:33.273951050 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 18:45:34.842943862 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:45:34.857267297 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:45:34.857307149 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 18:45:35.922566507 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:45:35.936226611 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:45:35.936251092 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 18:45:37.037805776 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:45:37.051465350 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:45:37.051488471 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 18:45:38.483669093 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:45:38.498454239 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:45:38.498485360 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~30000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~30000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 19999/20000 [05:26<00:00, 61.18it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1999/2000.0 [03:03<00:00, 10.91it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1999/2000.0 [03:00<00:00, 11.08it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7287499904632568
Final Model Recall: 0.45750001072883606
Final Model False Positives per Hour: 0.0
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754074687.795429  131912 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754074687.802464  131912 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754074687.823401  131912 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754074687.823436  131912 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754074687.823441  131912 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754074687.823447  131912 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  135091373427024: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  135091373426832: TensorSpec(shape=(), dtype=tf.float32, name=None)
  135091373428560: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  135091373428752: TensorSpec(shape=(), dtype=tf.float32, name=None)
  135091373431440: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  135091373429328: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  135091373428944: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  135091373425680: TensorSpec(shape=(), dtype=tf.float32, name=None)
  135091373429520: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  135091373432016: TensorSpec(shape=(), dtype=tf.float32, name=None)
  135091373431056: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  135091373429712: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  135091373427216: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  135091373429136: TensorSpec(shape=(), dtype=tf.float32, name=None)
  135091373430864: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754074692.558318  131912 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754074692.558628  131912 single_machine.cc:374] Starting new session
W0000 00:00:1754074692.613033  131912 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754074692.613079  131912 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754074692.676608  131912 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754074692.676798  131912 single_machine.cc:374] Starting new session
W0000 00:00:1754074692.724589  131912 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754074692.724633  131912 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 5/15 ===
Steps: 20000, N_samples: 35000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 18:58:23.460628930 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:58:23.474154169 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:58:23.474176309 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 18:58:25.091452593 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:58:25.105935156 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:58:25.105960006 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 18:58:26.225272128 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:58:26.244692462 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:58:26.244716952 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 18:58:27.375053490 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:58:27.388799345 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:58:27.388818795 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 18:58:28.706831956 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 18:58:28.721524515 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 18:58:28.721554245 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~35000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~35000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 19999/20000 [05:50<00:00, 57.02it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1999/2000.0 [02:48<00:00, 11.85it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1999/2000.0 [02:48<00:00, 11.87it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.750249981880188
Final Model Recall: 0.5005000233650208
Final Model False Positives per Hour: 0.0
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754075452.820044  139972 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754075452.826933  139972 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754075452.845421  139972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754075452.845460  139972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754075452.845465  139972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754075452.845471  139972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  137083548731536: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  137083548731920: TensorSpec(shape=(), dtype=tf.float32, name=None)
  137083548733456: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  137083548734032: TensorSpec(shape=(), dtype=tf.float32, name=None)
  137083548731728: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  137083548733648: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  137083548734224: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  137083548734608: TensorSpec(shape=(), dtype=tf.float32, name=None)
  137083548732112: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  137083548734416: TensorSpec(shape=(), dtype=tf.float32, name=None)
  137083548734800: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  137083548732688: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  137083548735952: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  137083548729616: TensorSpec(shape=(), dtype=tf.float32, name=None)
  137083548735760: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754075457.586825  139972 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754075457.587140  139972 single_machine.cc:374] Starting new session
W0000 00:00:1754075457.647014  139972 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754075457.647064  139972 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754075457.710757  139972 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754075457.710998  139972 single_machine.cc:374] Starting new session
W0000 00:00:1754075457.767883  139972 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754075457.767929  139972 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 6/15 ===
Steps: 20000, N_samples: 40000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 19:11:08.681538244 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:11:08.695511934 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:11:08.695540655 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 19:11:10.211159514 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:11:10.225708372 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:11:10.225741262 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 19:11:11.235668089 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:11:11.248596928 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:11:11.248621669 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 19:11:12.352179292 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:11:12.369657828 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:11:12.369691279 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 19:11:13.681730319 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:11:13.696709540 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:11:13.696745151 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~40000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~40000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 19999/20000 [05:24<00:00, 61.64it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1999/2000.0 [02:59<00:00, 11.13it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 1999/2000.0 [02:50<00:00, 11.70it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7432500123977661
Final Model Recall: 0.48649999499320984
Final Model False Positives per Hour: 0.0
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754076201.259278  148019 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754076201.266378  148019 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754076201.286847  148019 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754076201.286884  148019 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754076201.286889  148019 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754076201.286898  148019 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  125925088512144: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  125925088512528: TensorSpec(shape=(), dtype=tf.float32, name=None)
  125925088514064: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  125925088514640: TensorSpec(shape=(), dtype=tf.float32, name=None)
  125925088512336: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  125925088514256: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  125925088514832: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  125925088515216: TensorSpec(shape=(), dtype=tf.float32, name=None)
  125925088512720: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  125925088515024: TensorSpec(shape=(), dtype=tf.float32, name=None)
  125925088515408: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  125925088513296: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  125925088516560: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  125925088510224: TensorSpec(shape=(), dtype=tf.float32, name=None)
  125925088516368: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754076205.753247  148019 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754076205.753543  148019 single_machine.cc:374] Starting new session
W0000 00:00:1754076205.806801  148019 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754076205.806845  148019 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754076205.871184  148019 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754076205.871416  148019 single_machine.cc:374] Starting new session
W0000 00:00:1754076205.923151  148019 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754076205.923199  148019 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 7/15 ===
Steps: 25000, N_samples: 30000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 19:23:36.540041155 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:23:36.556896020 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:23:36.556921481 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 19:23:38.124932500 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:23:38.139827462 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:23:38.139860653 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 19:23:39.241960426 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:23:39.254752928 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:23:39.254774319 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 19:23:40.427630574 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:23:40.441313573 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:23:40.441336824 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 19:23:41.901554421 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:23:41.918278350 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:23:41.918312521 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~30000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~30000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 24999/25000 [06:25<00:00, 64.78it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2499/2500.0 [02:51<00:00, 14.60it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2499/2500.0 [02:54<00:00, 14.30it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7415000200271606
Final Model Recall: 0.4830000102519989
Final Model False Positives per Hour: 0.0
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754077008.866004  156004 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754077008.873156  156004 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754077008.893903  156004 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754077008.893936  156004 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754077008.893941  156004 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754077008.893947  156004 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  131537652402320: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  131537652402704: TensorSpec(shape=(), dtype=tf.float32, name=None)
  131537652404240: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  131537652404816: TensorSpec(shape=(), dtype=tf.float32, name=None)
  131537652402512: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  131537652404432: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  131537652405008: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  131537652405392: TensorSpec(shape=(), dtype=tf.float32, name=None)
  131537652402896: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  131537652405200: TensorSpec(shape=(), dtype=tf.float32, name=None)
  131537652405584: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  131537652403472: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  131537652406736: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  131537652400400: TensorSpec(shape=(), dtype=tf.float32, name=None)
  131537652406544: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754077013.548939  156004 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754077013.549184  156004 single_machine.cc:374] Starting new session
W0000 00:00:1754077013.602206  156004 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754077013.602252  156004 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754077013.665867  156004 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754077013.666059  156004 single_machine.cc:374] Starting new session
W0000 00:00:1754077013.717713  156004 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754077013.717768  156004 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 8/15 ===
Steps: 25000, N_samples: 35000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 19:37:04.293550710 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:37:04.307283310 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:37:04.307312231 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 19:37:05.837223488 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:37:05.851460057 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:37:05.851488578 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 19:37:06.906128708 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:37:06.919013466 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:37:06.919032877 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 19:37:08.123515659 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:37:08.137963925 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:37:08.137985626 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 19:37:09.593661791 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:37:09.609175647 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:37:09.609206628 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~35000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~35000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 24999/25000 [06:20<00:00, 65.63it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2499/2500.0 [02:55<00:00, 14.21it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2499/2500.0 [03:03<00:00, 13.62it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7622500061988831
Final Model Recall: 0.5245000123977661
Final Model False Positives per Hour: 0.17699114978313446
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754077825.784271  163829 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754077825.791276  163829 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754077825.811673  163829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754077825.811708  163829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754077825.811714  163829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754077825.811719  163829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  132431688698192: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  132431688698000: TensorSpec(shape=(), dtype=tf.float32, name=None)
  132431688699728: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  132431688699920: TensorSpec(shape=(), dtype=tf.float32, name=None)
  132431688702608: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  132431688700496: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  132431688700112: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  132431688696848: TensorSpec(shape=(), dtype=tf.float32, name=None)
  132431688700688: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  132431688703184: TensorSpec(shape=(), dtype=tf.float32, name=None)
  132431688702224: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  132431688700880: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  132431688698384: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  132431688700304: TensorSpec(shape=(), dtype=tf.float32, name=None)
  132431688702032: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754077830.603352  163829 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754077830.603645  163829 single_machine.cc:374] Starting new session
W0000 00:00:1754077830.658065  163829 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754077830.658113  163829 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754077830.722887  163829 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754077830.723104  163829 single_machine.cc:374] Starting new session
W0000 00:00:1754077830.770990  163829 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754077830.771036  163829 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 9/15 ===
Steps: 25000, N_samples: 40000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 19:50:41.536521284 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:50:41.550717736 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:50:41.550741137 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 19:50:43.033443775 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:50:43.052140149 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:50:43.052181981 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 19:50:44.170681277 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:50:44.184343349 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:50:44.184365150 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 19:50:45.236267865 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:50:45.251311777 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:50:45.251342728 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 19:50:46.611088941 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 19:50:46.627341426 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 19:50:46.627374008 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~40000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~40000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 24999/25000 [06:11<00:00, 67.36it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2499/2500.0 [02:58<00:00, 13.99it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2499/2500.0 [02:55<00:00, 14.26it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7574999928474426
Final Model Recall: 0.5149999856948853
Final Model False Positives per Hour: 0.17699114978313446
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754078628.925475  171659 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754078628.932326  171659 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754078628.951021  171659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754078628.951050  171659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754078628.951056  171659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754078628.951061  171659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  126289131964560: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  126289131964944: TensorSpec(shape=(), dtype=tf.float32, name=None)
  126289131966480: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  126289131967056: TensorSpec(shape=(), dtype=tf.float32, name=None)
  126289131964752: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  126289131966672: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  126289131967248: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  126289131967632: TensorSpec(shape=(), dtype=tf.float32, name=None)
  126289131965136: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  126289131967440: TensorSpec(shape=(), dtype=tf.float32, name=None)
  126289131967824: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  126289131965712: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  126289131968976: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  126289131962640: TensorSpec(shape=(), dtype=tf.float32, name=None)
  126289131968784: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754078633.540414  171659 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754078633.540660  171659 single_machine.cc:374] Starting new session
W0000 00:00:1754078633.593566  171659 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754078633.593603  171659 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754078633.655403  171659 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754078633.655574  171659 single_machine.cc:374] Starting new session
W0000 00:00:1754078633.706629  171659 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754078633.706666  171659 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 10/15 ===
Steps: 30000, N_samples: 30000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 20:04:04.264557604 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:04:04.277945511 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:04:04.277967193 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 20:04:05.820710760 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:04:05.834449360 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:04:05.834476912 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 20:04:06.821617152 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:04:06.834382198 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:04:06.834403730 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 20:04:07.930684924 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:04:07.945423186 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:04:07.945447887 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 20:04:09.397080484 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:04:09.412270114 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:04:09.412293106 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~30000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~30000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 29999/30000 [06:49<00:00, 73.26it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2999/3000.0 [03:11<00:00, 15.65it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2999/3000.0 [02:56<00:00, 17.01it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7247499823570251
Final Model Recall: 0.4494999945163727
Final Model False Positives per Hour: 0.0
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754079483.023294  179384 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754079483.030083  179384 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754079483.048506  179384 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754079483.048537  179384 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754079483.048542  179384 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754079483.048547  179384 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  134241755760976: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  134241755760784: TensorSpec(shape=(), dtype=tf.float32, name=None)
  134241755762512: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  134241755762704: TensorSpec(shape=(), dtype=tf.float32, name=None)
  134241755765392: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  134241755763280: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  134241755762896: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  134241755759632: TensorSpec(shape=(), dtype=tf.float32, name=None)
  134241755763472: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  134241755765968: TensorSpec(shape=(), dtype=tf.float32, name=None)
  134241755765008: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  134241755763664: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  134241755761168: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  134241755763088: TensorSpec(shape=(), dtype=tf.float32, name=None)
  134241755764816: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754079487.583721  179384 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754079487.583966  179384 single_machine.cc:374] Starting new session
W0000 00:00:1754079487.636018  179384 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754079487.636054  179384 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754079487.697068  179384 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754079487.697246  179384 single_machine.cc:374] Starting new session
W0000 00:00:1754079487.744186  179384 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754079487.744217  179384 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 11/15 ===
Steps: 30000, N_samples: 35000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 20:18:18.256017924 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:18:18.270932323 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:18:18.270955204 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 20:18:19.847642808 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:18:19.872634590 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:18:19.872658641 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 20:18:20.958650447 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:18:20.972915155 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:18:20.972935796 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 20:18:22.037321038 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:18:22.050795876 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:18:22.050816037 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 20:18:23.424184711 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:18:23.438837667 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:18:23.438862538 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~35000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~35000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 29999/30000 [06:45<00:00, 73.99it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2999/3000.0 [03:20<00:00, 14.95it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2999/3000.0 [03:00<00:00, 16.61it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7590000033378601
Final Model Recall: 0.5180000066757202
Final Model False Positives per Hour: 0.0
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754080346.331385  187089 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754080346.338440  187089 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754080346.359185  187089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754080346.359222  187089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754080346.359228  187089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754080346.359233  187089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  123185327917200: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  123185327917584: TensorSpec(shape=(), dtype=tf.float32, name=None)
  123185327919120: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  123185327919696: TensorSpec(shape=(), dtype=tf.float32, name=None)
  123185327917392: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  123185327919312: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  123185327919888: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  123185327920272: TensorSpec(shape=(), dtype=tf.float32, name=None)
  123185327917776: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  123185327920080: TensorSpec(shape=(), dtype=tf.float32, name=None)
  123185327920464: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  123185327918352: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  123185327921616: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  123185327915280: TensorSpec(shape=(), dtype=tf.float32, name=None)
  123185327921424: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754080351.077869  187089 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754080351.078126  187089 single_machine.cc:374] Starting new session
W0000 00:00:1754080351.131153  187089 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754080351.131204  187089 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754080351.196205  187089 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754080351.196448  187089 single_machine.cc:374] Starting new session
W0000 00:00:1754080351.248036  187089 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754080351.248088  187089 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 12/15 ===
Steps: 30000, N_samples: 40000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 20:32:41.962694238 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:32:41.976216094 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:32:41.976238294 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 20:32:43.647870617 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:32:43.662642566 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:32:43.662669786 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 20:32:44.734125716 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:32:44.747414319 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:32:44.747441520 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 20:32:45.910956867 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:32:45.926883568 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:32:45.926908598 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 20:32:47.318096199 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:32:47.333786538 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:32:47.333816078 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~40000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~40000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 2999/3000.0 [02:50<00:00, 17.56it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 2999/3000.0 [02:51<00:00, 17.46it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7562500238418579
Final Model Recall: 0.512499988079071
Final Model False Positives per Hour: 0.0
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754081179.012775  194798 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754081179.019657  194798 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754081179.038442  194798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754081179.038471  194798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754081179.038482  194798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754081179.038490  194798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  132058760464528: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  132058760464912: TensorSpec(shape=(), dtype=tf.float32, name=None)
  132058760466448: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  132058760467024: TensorSpec(shape=(), dtype=tf.float32, name=None)
  132058760464720: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  132058760466640: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  132058760467216: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  132058760467600: TensorSpec(shape=(), dtype=tf.float32, name=None)
  132058760465104: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  132058760467408: TensorSpec(shape=(), dtype=tf.float32, name=None)
  132058760467792: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  132058760465680: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  132058760468944: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  132058760462608: TensorSpec(shape=(), dtype=tf.float32, name=None)
  132058760468752: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754081183.540688  194798 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754081183.540947  194798 single_machine.cc:374] Starting new session
W0000 00:00:1754081183.592984  194798 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754081183.593021  194798 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754081183.653654  194798 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754081183.653830  194798 single_machine.cc:374] Starting new session
W0000 00:00:1754081183.704987  194798 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754081183.705028  194798 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 13/15 ===
Steps: 35000, N_samples: 30000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 20:46:34.147831828 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:46:34.161870410 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:46:34.161896271 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 20:46:35.791325428 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:46:35.805134522 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:46:35.805160363 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 20:46:36.838239676 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:46:36.851201309 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:46:36.851223960 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 20:46:38.009718411 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:46:38.024405177 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:46:38.024426778 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 20:46:39.397242670 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 20:46:39.412564260 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 20:46:39.412606032 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~30000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~30000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 34999/35000 [07:44<00:00, 75.42it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 3499/3500.0 [03:18<00:00, 17.66it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 3499/3500.0 [02:57<00:00, 19.68it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7882500290870667
Final Model Recall: 0.5764999985694885
Final Model False Positives per Hour: 0.3539822995662689
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754082097.495435  202497 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754082097.502592  202497 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754082097.523213  202497 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754082097.523251  202497 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754082097.523256  202497 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754082097.523262  202497 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  126454287632720: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  126454287632528: TensorSpec(shape=(), dtype=tf.float32, name=None)
  126454287634256: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  126454287634448: TensorSpec(shape=(), dtype=tf.float32, name=None)
  126454287637136: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  126454287635024: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  126454287634640: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  126454287631376: TensorSpec(shape=(), dtype=tf.float32, name=None)
  126454287635216: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  126454287637712: TensorSpec(shape=(), dtype=tf.float32, name=None)
  126454287636752: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  126454287635408: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  126454287632912: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  126454287634832: TensorSpec(shape=(), dtype=tf.float32, name=None)
  126454287636560: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754082102.189380  202497 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754082102.189682  202497 single_machine.cc:374] Starting new session
W0000 00:00:1754082102.246020  202497 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754082102.246068  202497 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754082102.309941  202497 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754082102.310149  202497 single_machine.cc:374] Starting new session
W0000 00:00:1754082102.357608  202497 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754082102.357654  202497 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 14/15 ===
Steps: 35000, N_samples: 35000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 21:01:52.748354164 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 21:01:52.761711033 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 21:01:52.761732134 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 21:01:54.304730477 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 21:01:54.318722352 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 21:01:54.318747513 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 21:01:55.332412638 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 21:01:55.346857991 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 21:01:55.346893803 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 21:01:56.468234455 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 21:01:56.482565343 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 21:01:56.482592474 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 21:01:57.809021616 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 21:01:57.823672717 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 21:01:57.823695668 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~35000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~35000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training: 100%|██████████████████████████▉| 34999/35000 [07:39<00:00, 76.10it/s]
INFO:root:##################################################
Starting training sequence 2...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 3499/3500.0 [03:09<00:00, 18.48it/s]
INFO:root:##################################################
Starting training sequence 3...
##################################################
INFO:root:Increasing weight on negative examples to reduce false positives...
Training: 100%|██████████████████████████▉| 3499/3500.0 [03:16<00:00, 17.82it/s]
INFO:root:Merging checkpoints above the 90th percentile into single model...
INFO:root:
################
Final Model Accuracy: 0.7735000252723694
Final Model Recall: 0.546999990940094
Final Model False Positives per Hour: 0.17699114978313446
################

INFO:root:####
Saving ONNX mode as '/workspace/combs/final_result/final_result.onnx'
Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: tf_keras in /opt/conda/lib/python3.11/site-packages (2.19.0)
Requirement already satisfied: ai_edge_litert in /opt/conda/lib/python3.11/site-packages (1.4.0)
Requirement already satisfied: onnxsim in /opt/conda/lib/python3.11/site-packages (0.4.36)
Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.3.1)
Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.5)
Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)
Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.74.0)
Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.19.0)
Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.1)
Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.2)
Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.14.0)
Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)
Requirement already satisfied: backports.strenum in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (1.2.8)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from ai_edge_litert) (4.66.5)
Requirement already satisfied: onnx in /opt/conda/lib/python3.11/site-packages (from onnxsim) (1.18.0)
Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from onnxsim) (14.1.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)
Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)
Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)
Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->onnxsim) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1754083021.815684  210200 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1754083021.822718  210200 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1754083021.841539  210200 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754083021.841565  210200 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754083021.841570  210200 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1754083021.841576  210200 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.

Model optimizing started ============================================================
Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 14             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 200.6KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Simplifying...
Finish! Here is the difference:
┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃            ┃ Original Model ┃ Simplified Model ┃
┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Add        │ 4              │ 4                │
│ Constant   │ 12             │ 12               │
│ Div        │ 2              │ 2                │
│ Flatten    │ 1              │ 1                │
│ Gemm       │ 3              │ 3                │
│ Mul        │ 2              │ 2                │
│ Pow        │ 2              │ 2                │
│ ReduceMean │ 4              │ 4                │
│ Relu       │ 2              │ 2                │
│ Sigmoid    │ 1              │ 1                │
│ Sqrt       │ 2              │ 2                │
│ Sub        │ 2              │ 2                │
│ Model Size │ 201.4KiB       │ 201.4KiB         │
└────────────┴────────────────┴──────────────────┘

Model optimizing complete!

Automatic generation of each OP name started ========================================
Automatic generation of each OP name complete!

Model loaded ========================================================================

Model conversion started ============================================================
INFO: input_op_name: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32

INFO: 2 / 26
INFO: onnx_op_type: Flatten onnx_op_name: /flatten/Flatten
INFO:  input_name.1: onnx____Flatten_0 shape: [1, 16, 96] dtype: float32
INFO:  output_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO: tf_op_type: reshape
INFO:  input.1.tensor: name: onnx____Flatten_0 shape: (1, 16, 96) dtype: <dtype: 'float32'> 
INFO:  input.2.shape: val: [1, 1536] 
INFO:  output.1.output: name: tf.reshape/Reshape:0 shape: (1, 1536) dtype: <dtype: 'float32'> 

INFO: 3 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /layer1/Gemm
INFO:  input_name.1: /flatten/Flatten_output_0 shape: [1, 1536] dtype: float32
INFO:  input_name.2: layer1.weight shape: [32, 1536] dtype: float32
INFO:  input_name.3: layer1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 1536) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1536, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 4 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 5 / 26
INFO: onnx_op_type: Sub onnx_op_name: /layernorm1/Sub
INFO:  input_name.1: /layer1/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_1/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 6 / 26
INFO: onnx_op_type: Pow onnx_op_name: /layernorm1/Pow
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 7 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /layernorm1/ReduceMean_1
INFO:  input_name.1: /layernorm1/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_4/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 8 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add
INFO:  input_name.1: /layernorm1/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: [] dtype: float32
INFO:  output_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_3/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 9 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /layernorm1/Sqrt
INFO:  input_name.1: /layernorm1/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_1/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 10 / 26
INFO: onnx_op_type: Div onnx_op_name: /layernorm1/Div
INFO:  input_name.1: /layernorm1/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 11 / 26
INFO: onnx_op_type: Mul onnx_op_name: /layernorm1/Mul
INFO:  input_name.1: /layernorm1/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.weight shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 12 / 26
INFO: onnx_op_type: Add onnx_op_name: /layernorm1/Add_1
INFO:  input_name.1: /layernorm1/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: layernorm1.bias shape: [32] dtype: float32
INFO:  output_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_10/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 13 / 26
INFO: onnx_op_type: Relu onnx_op_name: /relu1/Relu
INFO:  input_name.1: /layernorm1/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_2/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 14 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /blocks.0/fcn_layer/Gemm
INFO:  input_name.1: /relu1/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.fcn_layer.weight shape: [32, 32] dtype: float32
INFO:  input_name.3: blocks.0.fcn_layer.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (32,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 15 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 16 / 26
INFO: onnx_op_type: Sub onnx_op_name: /blocks.0/layer_norm/Sub
INFO:  input_name.1: /blocks.0/fcn_layer/Gemm_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/ReduceMean_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: subtract
INFO:  input.1.x: name: tf.__operators__.add_1/AddV2:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.reduce_mean_5/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 17 / 26
INFO: onnx_op_type: Pow onnx_op_name: /blocks.0/layer_norm/Pow
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: pow
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 18 / 26
INFO: onnx_op_type: ReduceMean onnx_op_name: /blocks.0/layer_norm/ReduceMean_1
INFO:  input_name.1: /blocks.0/layer_norm/Pow_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: reduce_mean
INFO:  input.1.axis0: val: 1 
INFO:  input.2.input_tensor: name: tf.math.multiply_16/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.3.keepdims: val: True 
INFO:  output.1.output: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 19 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add
INFO:  input_name.1: /blocks.0/layer_norm/ReduceMean_1_output_0 shape: [1, 1] dtype: float32
INFO:  input_name.2: /layernorm1/Constant_1_output_0 shape: () dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.reduce_mean_7/Mean:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: () dtype: float32 
INFO:  output.1.output: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 20 / 26
INFO: onnx_op_type: Sqrt onnx_op_name: /blocks.0/layer_norm/Sqrt
INFO:  input_name.1: /blocks.0/layer_norm/Add_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: sqrt
INFO:  input.1.x: name: tf.math.add_4/Add:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 21 / 26
INFO: onnx_op_type: Div onnx_op_name: /blocks.0/layer_norm/Div
INFO:  input_name.1: /blocks.0/layer_norm/Sub_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: /blocks.0/layer_norm/Sqrt_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: divide
INFO:  input.1.x: name: tf.math.subtract_1/Sub:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: name: tf.math.sqrt_1/Sqrt:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 22 / 26
INFO: onnx_op_type: Mul onnx_op_name: /blocks.0/layer_norm/Mul
INFO:  input_name.1: /blocks.0/layer_norm/Div_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.weight shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: multiply
INFO:  input.1.x: name: tf.math.divide_1/truediv:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 23 / 26
INFO: onnx_op_type: Add onnx_op_name: /blocks.0/layer_norm/Add_1
INFO:  input_name.1: /blocks.0/layer_norm/Mul_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: blocks.0.layer_norm.bias shape: [32] dtype: float32
INFO:  output_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: add
INFO:  input.1.x: name: tf.math.multiply_22/Mul:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 24 / 26
INFO: onnx_op_type: Relu onnx_op_name: /blocks.0/relu/Relu
INFO:  input_name.1: /blocks.0/layer_norm/Add_1_output_0 shape: [1, 32] dtype: float32
INFO:  output_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO: tf_op_type: relu
INFO:  input.1.features: name: tf.math.add_5/Add:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.nn.relu_1/Relu:0 shape: (1, 32) dtype: <dtype: 'float32'> 

INFO: 25 / 26
INFO: onnx_op_type: Gemm onnx_op_name: /last_layer/Gemm
INFO:  input_name.1: /blocks.0/relu/Relu_output_0 shape: [1, 32] dtype: float32
INFO:  input_name.2: last_layer.weight shape: [1, 32] dtype: float32
INFO:  input_name.3: last_layer.bias shape: [1] dtype: float32
INFO:  output_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO: tf_op_type: matmul
INFO:  input.1.x: name: Placeholder:0 shape: (1, 32) dtype: <dtype: 'float32'> 
INFO:  input.2.y: shape: (32, 1) dtype: <dtype: 'float32'> 
INFO:  input.3.z: shape: (1,) dtype: <dtype: 'float32'> 
INFO:  input.4.alpha: val: 1.0 
INFO:  input.5.beta: val: 1.0 
INFO:  output.1.output: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 

INFO: 26 / 26
INFO: onnx_op_type: Sigmoid onnx_op_name: /last_act/Sigmoid
INFO:  input_name.1: /last_layer/Gemm_output_0 shape: [1, 1] dtype: float32
INFO:  output_name.1: 39 shape: [1, 1] dtype: float32
INFO: tf_op_type: sigmoid
INFO:  input.1.x: name: tf.__operators__.add_2/AddV2:0 shape: (1, 1) dtype: <dtype: 'float32'> 
INFO:  output.1.output: name: tf.math.sigmoid/Sigmoid:0 shape: (1, 1) dtype: <dtype: 'float32'> 

saved_model output started ==========================================================
Saved artifact at 'final_result/'. The following endpoints are available:

* Endpoint 'serving_default'
  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 16, 96), dtype=tf.float32, name='onnx____Flatten_0')
Output Type:
  TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)
Captures:
  124611310373200: TensorSpec(shape=(1536, 32), dtype=tf.float32, name=None)
  124611310373008: TensorSpec(shape=(), dtype=tf.float32, name=None)
  124611310374736: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  124611310374928: TensorSpec(shape=(), dtype=tf.float32, name=None)
  124611310377616: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  124611310375504: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  124611310375120: TensorSpec(shape=(32, 32), dtype=tf.float32, name=None)
  124611310371856: TensorSpec(shape=(), dtype=tf.float32, name=None)
  124611310375696: TensorSpec(shape=(32,), dtype=tf.float32, name=None)
  124611310378192: TensorSpec(shape=(), dtype=tf.float32, name=None)
  124611310377232: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  124611310375888: TensorSpec(shape=(1, 32), dtype=tf.float32, name=None)
  124611310373392: TensorSpec(shape=(32, 1), dtype=tf.float32, name=None)
  124611310375312: TensorSpec(shape=(), dtype=tf.float32, name=None)
  124611310377040: TensorSpec(shape=(1,), dtype=tf.float32, name=None)
saved_model output complete!
I0000 00:00:1754083026.518519  210200 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754083026.518751  210200 single_machine.cc:374] Starting new session
W0000 00:00:1754083026.575118  210200 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754083026.575153  210200 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float32 tflite output complete!
I0000 00:00:1754083026.636483  210200 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
I0000 00:00:1754083026.636698  210200 single_machine.cc:374] Starting new session
W0000 00:00:1754083026.683838  210200 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1754083026.683873  210200 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
Float16 tflite output complete!

=== Training combination 15/15 ===
Steps: 35000, N_samples: 40000, Max_negative_weight: 3000
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
✓ pt_PT-tugão-medium.onnx already exists, skipping download
✓ pt_PT-tugão-medium.onnx.json already exists, skipping download
Ensuring voice exists: pt_PT-tugão-medium
✓ Voice 'pt_PT-tugão-medium' is ready.
2025-08-01 21:17:17.465419765 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 21:17:17.479398008 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 21:17:17.479423518 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: es_MX-claude-high
✓ Voice 'es_MX-claude-high' is ready.
2025-08-01 21:17:19.058632516 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 21:17:19.075054908 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 21:17:19.075084658 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-cadu-medium
✓ Voice 'pt_BR-cadu-medium' is ready.
2025-08-01 21:17:20.172424245 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch-jit-export for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 21:17:20.185348649 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 21:17:20.185368799 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_BR-faber-medium
✓ Voice 'pt_BR-faber-medium' is ready.
2025-08-01 21:17:21.341468990 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 21:17:21.355059957 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 21:17:21.355080917 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
Ensuring voice exists: pt_PT-rita
✗ Voice 'pt_PT-rita' not found in voices.json. Checking local models...
✓ Found local model: models/pt_PT-rita.onnx
2025-08-01 21:17:22.745069908 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 28 Memcpy nodes are added to the graph torch_jit for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.
2025-08-01 21:17:22.759850738 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.
2025-08-01 21:17:22.759874499 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.
✓ Successfully loaded local model 'pt_PT-rita'
✅ Loaded 5 voice(s)
  0: pt
  1: es-419
  2: pt-br
  3: pt-br
  4: pt
INFO:root:##################################################
Generating positive clips for training
##################################################
WARNING:root:Skipping generation of positive clips for training, as ~40000 already exist
INFO:root:##################################################
Generating positive clips for testing
##################################################
WARNING:root:Skipping generation of positive clips testing, as ~2000 already exist
INFO:root:##################################################
Generating negative clips for training
##################################################
WARNING:root:Skipping generation of negative clips for training, as ~40000 already exist
INFO:root:##################################################
Generating negative clips for testing
##################################################
WARNING:root:Skipping generation of negative clips for testing, as ~2000 already exist
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
WARNING:root:Openwakeword features already exist, skipping data augmentation and feature generation
/opt/conda/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.
  torchaudio.set_audio_backend("soundfile")
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
INFO:root:##################################################
Starting training sequence 1...
##################################################
Training:  14%|███▊                       | 4965/35000 [00:48<04:43, 105.91it/s]